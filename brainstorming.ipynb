{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/100, Total Reward: 72\n",
      "Episode 2/100, Total Reward: 71\n",
      "Episode 3/100, Total Reward: 68\n",
      "Episode 4/100, Total Reward: 76\n",
      "Episode 5/100, Total Reward: 66\n",
      "Episode 6/100, Total Reward: 69\n",
      "Episode 7/100, Total Reward: 65\n",
      "Episode 8/100, Total Reward: 78\n",
      "Episode 9/100, Total Reward: 75\n",
      "Episode 10/100, Total Reward: 77\n",
      "Episode 11/100, Total Reward: 67\n",
      "Episode 12/100, Total Reward: 72\n",
      "Episode 13/100, Total Reward: 64\n",
      "Episode 14/100, Total Reward: 69\n",
      "Episode 15/100, Total Reward: 77\n",
      "Episode 16/100, Total Reward: 77\n",
      "Episode 17/100, Total Reward: 74\n",
      "Episode 18/100, Total Reward: 62\n",
      "Episode 19/100, Total Reward: 68\n",
      "Episode 20/100, Total Reward: 77\n",
      "Episode 21/100, Total Reward: 69\n",
      "Episode 22/100, Total Reward: 70\n",
      "Episode 23/100, Total Reward: 75\n",
      "Episode 24/100, Total Reward: 70\n",
      "Episode 25/100, Total Reward: 67\n",
      "Episode 26/100, Total Reward: 73\n",
      "Episode 27/100, Total Reward: 73\n",
      "Episode 28/100, Total Reward: 67\n",
      "Episode 29/100, Total Reward: 69\n",
      "Episode 30/100, Total Reward: 59\n",
      "Episode 31/100, Total Reward: 63\n",
      "Episode 32/100, Total Reward: 65\n",
      "Episode 33/100, Total Reward: 78\n",
      "Episode 34/100, Total Reward: 73\n",
      "Episode 35/100, Total Reward: 72\n",
      "Episode 36/100, Total Reward: 72\n",
      "Episode 37/100, Total Reward: 72\n",
      "Episode 38/100, Total Reward: 70\n",
      "Episode 39/100, Total Reward: 64\n",
      "Episode 40/100, Total Reward: 71\n",
      "Episode 41/100, Total Reward: 71\n",
      "Episode 42/100, Total Reward: 68\n",
      "Episode 43/100, Total Reward: 68\n",
      "Episode 44/100, Total Reward: 73\n",
      "Episode 45/100, Total Reward: 73\n",
      "Episode 46/100, Total Reward: 76\n",
      "Episode 47/100, Total Reward: 68\n",
      "Episode 48/100, Total Reward: 70\n",
      "Episode 49/100, Total Reward: 69\n",
      "Episode 50/100, Total Reward: 63\n",
      "Episode 51/100, Total Reward: 78\n",
      "Episode 52/100, Total Reward: 72\n",
      "Episode 53/100, Total Reward: 68\n",
      "Episode 54/100, Total Reward: 75\n",
      "Episode 55/100, Total Reward: 79\n",
      "Episode 56/100, Total Reward: 74\n",
      "Episode 57/100, Total Reward: 68\n",
      "Episode 58/100, Total Reward: 67\n",
      "Episode 59/100, Total Reward: 73\n",
      "Episode 60/100, Total Reward: 65\n",
      "Episode 61/100, Total Reward: 77\n",
      "Episode 62/100, Total Reward: 74\n",
      "Episode 63/100, Total Reward: 64\n",
      "Episode 64/100, Total Reward: 72\n",
      "Episode 65/100, Total Reward: 69\n",
      "Episode 66/100, Total Reward: 72\n",
      "Episode 67/100, Total Reward: 68\n",
      "Episode 68/100, Total Reward: 66\n",
      "Episode 69/100, Total Reward: 67\n",
      "Episode 70/100, Total Reward: 61\n",
      "Episode 71/100, Total Reward: 71\n",
      "Episode 72/100, Total Reward: 67\n",
      "Episode 73/100, Total Reward: 72\n",
      "Episode 74/100, Total Reward: 71\n",
      "Episode 75/100, Total Reward: 74\n",
      "Episode 76/100, Total Reward: 73\n",
      "Episode 77/100, Total Reward: 69\n",
      "Episode 78/100, Total Reward: 69\n",
      "Episode 79/100, Total Reward: 69\n",
      "Episode 80/100, Total Reward: 70\n",
      "Episode 81/100, Total Reward: 71\n",
      "Episode 82/100, Total Reward: 68\n",
      "Episode 83/100, Total Reward: 67\n",
      "Episode 84/100, Total Reward: 69\n",
      "Episode 85/100, Total Reward: 70\n",
      "Episode 86/100, Total Reward: 66\n",
      "Episode 87/100, Total Reward: 80\n",
      "Episode 88/100, Total Reward: 70\n",
      "Episode 89/100, Total Reward: 62\n",
      "Episode 90/100, Total Reward: 71\n",
      "Episode 91/100, Total Reward: 73\n",
      "Episode 92/100, Total Reward: 71\n",
      "Episode 93/100, Total Reward: 73\n",
      "Episode 94/100, Total Reward: 76\n",
      "Episode 95/100, Total Reward: 64\n",
      "Episode 96/100, Total Reward: 74\n",
      "Episode 97/100, Total Reward: 71\n",
      "Episode 98/100, Total Reward: 64\n",
      "Episode 99/100, Total Reward: 67\n",
      "Episode 100/100, Total Reward: 77\n",
      "Average reward over evaluation episodes: 73.28\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the number of contexts and actions\n",
    "num_contexts = 3\n",
    "num_actions = 2\n",
    "\n",
    "# Define the true reward probabilities for each context-action pair\n",
    "true_rewards = np.array([[0.9, 0.1], [0.2, 0.8], [0.5, 0.5]])\n",
    "\n",
    "class ContextualBandit:\n",
    "    def __init__(self, num_contexts, num_actions, true_rewards):\n",
    "        self.num_contexts = num_contexts\n",
    "        self.num_actions = num_actions\n",
    "        self.true_rewards = true_rewards\n",
    "\n",
    "    def get_reward(self, context, action):\n",
    "        reward_probability = self.true_rewards[context, action]\n",
    "        reward = 1 if np.random.rand() < reward_probability else 0\n",
    "        return reward\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, num_contexts, num_actions, learning_rate=0.1, epsilon=0.1):\n",
    "        self.num_contexts = num_contexts\n",
    "        self.num_actions = num_actions\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.q_table = np.zeros((num_contexts, num_actions))\n",
    "\n",
    "    def select_action(self, context, greedy=False):\n",
    "        if not greedy and np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        else:\n",
    "            return np.argmax(self.q_table[context])\n",
    "\n",
    "    def update_q_values(self, context, action, reward):\n",
    "        td_error = reward - self.q_table[context, action]\n",
    "        self.q_table[context, action] += self.learning_rate * td_error\n",
    "\n",
    "def train_agent(agent, bandit, num_episodes=100, steps_per_episode=100):\n",
    "    episode_rewards = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        total_reward = 0\n",
    "        for step in range(steps_per_episode):\n",
    "            context = np.random.choice(num_contexts)\n",
    "            action = agent.select_action(context)\n",
    "            reward = bandit.get_reward(context, action)\n",
    "            agent.update_q_values(context, action, reward)\n",
    "            total_reward += reward\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        print(f'Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward}')\n",
    "\n",
    "    return agent\n",
    "\n",
    "# Initialize bandit and agent\n",
    "bandit = ContextualBandit(num_contexts, num_actions, true_rewards)\n",
    "agent = QLearningAgent(num_contexts, num_actions)\n",
    "\n",
    "# Train the agent\n",
    "trained_agent = train_agent(agent, bandit)\n",
    "\n",
    "# Evaluate the trained agent\n",
    "def evaluate_agent(agent, bandit, num_episodes=100, steps_per_episode=100):\n",
    "    total_reward = 0\n",
    "    for episode in range(num_episodes):\n",
    "        for step in range(steps_per_episode):\n",
    "            context = np.random.choice(num_contexts)\n",
    "            action = agent.select_action(context, greedy=True)\n",
    "            reward = bandit.get_reward(context, action)\n",
    "            total_reward += reward\n",
    "\n",
    "    average_reward = total_reward / num_episodes\n",
    "    return average_reward\n",
    "\n",
    "average_reward = evaluate_agent(trained_agent, bandit)\n",
    "print(f'Average reward over evaluation episodes: {average_reward:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.91642892, 0.0390233 ],\n",
       "       [0.17823553, 0.93568616],\n",
       "       [0.38541945, 0.49716237]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.q_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent-room-env-v2-gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

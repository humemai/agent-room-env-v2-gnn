{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the results of handcrafted, baselines, and HumemAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tk/.virtualenvs/agent-room-env-v2-gnn/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "import shutil\n",
    "import os\n",
    "from glob import glob\n",
    "from humemai.utils import read_yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from typing import Literal\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_handcrafted(\n",
    "    size: Literal[\"xxs\", \"xs\", \"s\", \"m\", \"l\", \"xl\", \"xxl\"],\n",
    "    include_pretrain_semantic: bool = True,\n",
    "    base_path: str = \"training-results/non-equal-object-probs/\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Get hand-crafted results.\n",
    "\n",
    "    room_size=xxs   num_obs=6.0     max_obs=6   min_obs=6\n",
    "    room_size=xs    num_obs=6.52    max_obs=8   min_obs=5\n",
    "    room_size=s     num_obs=5.64    max_obs=7   min_obs=5\n",
    "    room_size=m     num_obs=6.3     max_obs=10  min_obs=5\n",
    "    room_size=l     num_obs=5.32    max_obs=8   min_obs=5\n",
    "    room_size=xl    num_obs=5.58    max_obs=7   min_obs=5\n",
    "    room_size=xxl   num_obs=6.0     max_obs=8   min_obs=5\n",
    "\n",
    "    Args:\n",
    "        size: room size\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(\n",
    "        os.path.join(\n",
    "            base_path, f\"handcrafted/hand-crafted-results-room_size={size}.csv\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if not include_pretrain_semantic:\n",
    "        df = df[\n",
    "            (df[\"pretrain_semantic\"] == False)\n",
    "            | (df[\"pretrain_semantic\"].str.lower() == \"false\")\n",
    "        ]\n",
    "\n",
    "    df[\"test_mean\"] = pd.to_numeric(df[\"test_mean\"], errors=\"coerce\")\n",
    "    df_sorted = df.sort_values(\n",
    "        by=[\"long_capacity\", \"test_mean\"], ascending=[True, False]\n",
    "    )\n",
    "    df = df_sorted.groupby(\"long_capacity\").first().reset_index()\n",
    "\n",
    "    # add column \"room_size\":\n",
    "    df[\"room_size\"] = size\n",
    "\n",
    "    # Rename columns\n",
    "    df.rename(\n",
    "        columns={\n",
    "            \"long_capacity\": \"capacity\",\n",
    "            \"test_mean\": \"test\",\n",
    "            \"test_std\": \"std_test\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    df[\"val\"] = np.nan\n",
    "    df[\"agent_type\"] = \"handcrafted\"\n",
    "    df[\"#_runs\"] = 5\n",
    "    df[\"terminates_at\"] = 99\n",
    "\n",
    "    # Reorder columns in df1\n",
    "    df = df[\n",
    "        [\n",
    "            \"test\",\n",
    "            # \"std_test\",\n",
    "            \"val\",\n",
    "            \"#_runs\",\n",
    "            \"capacity\",\n",
    "            \"agent_type\",\n",
    "            \"pretrain_semantic\",\n",
    "            \"semantic_decay_factor\",\n",
    "            \"room_size\",\n",
    "            \"explore_policy\",\n",
    "            \"qa_function\",\n",
    "            \"mm_policy\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    # rename columns\n",
    "    df.rename(columns={\"mm_policy\": \"mm\"}, inplace=True)\n",
    "    df.rename(columns={\"qa_function\": \"qa\"}, inplace=True)\n",
    "    df.rename(columns={\"explore_policy\": \"explore\"}, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def determine_hyper_parameters(train: dict) -> dict:\n",
    "    \"\"\"Determine hyper parameters.\"\"\"\n",
    "    hyper_parameters = {}\n",
    "\n",
    "    hyper_parameters[\"capacity\"] = train[\"capacity\"][\"long\"]\n",
    "    hyper_parameters[\"pretrain_semantic\"] = train[\"pretrain_semantic\"]\n",
    "    hyper_parameters[\"semantic_decay_factor\"] = train[\"semantic_decay_factor\"]\n",
    "    hyper_parameters[\"room_size\"] = train[\"env_config\"][\"room_size\"]\n",
    "    hyper_parameters[\"#_iter\"] = train[\"num_iterations\"]\n",
    "    hyper_parameters[\"replay_size\"] = train[\"replay_buffer_size\"]\n",
    "    # hyper_parameters[\"warm_start\"] = train[\"warm_start\"]\n",
    "    hyper_parameters[\"terminates_at\"] = train[\"env_config\"][\"terminates_at\"]\n",
    "    hyper_parameters[\"target_update\"] = train[\"target_update_interval\"]\n",
    "    if isinstance(train[\"gamma\"], dict):\n",
    "        hyper_parameters[\"gamma_mm\"] = train[\"gamma\"][\"mm\"]\n",
    "        hyper_parameters[\"gamma_explore\"] = train[\"gamma\"][\"explore\"]\n",
    "    else:\n",
    "        hyper_parameters[\"gamma_mm\"] = train[\"gamma\"]\n",
    "        hyper_parameters[\"gamma_explore\"] = train[\"gamma\"]\n",
    "\n",
    "    hyper_parameters[\"agent_type\"] = \"dqn\"\n",
    "    hyper_parameters[\"relu_between\"] = train[\"dqn_params\"][\"relu_between_gcn_layers\"]\n",
    "    hyper_parameters[\"dropout_between\"] = train[\"dqn_params\"][\n",
    "        \"dropout_between_gcn_layers\"\n",
    "    ]\n",
    "    hyper_parameters[\"num_layers\"] = train[\"dqn_params\"][\"gcn_layer_params\"][\n",
    "        \"num_layers\"\n",
    "    ]\n",
    "    hyper_parameters[\"batch_size\"] = train[\"batch_size\"]\n",
    "    hyper_parameters[\"emb_dim\"] = train[\"dqn_params\"][\"gcn_layer_params\"][\n",
    "        \"embedding_dim\"\n",
    "    ]\n",
    "    hyper_parameters[\"triple_weight\"] = train[\"dqn_params\"][\"gcn_layer_params\"][\n",
    "        \"triple_qual_weight\"\n",
    "    ]\n",
    "    hyper_parameters[\"intrinsic\"] = train[\"intrinsic_explore_reward\"]\n",
    "    hyper_parameters[\"mm\"] = train[\"mm_policy\"]\n",
    "    hyper_parameters[\"explore\"] = train[\"explore_policy\"]\n",
    "    hyper_parameters[\"gcn\"] = train[\"dqn_params\"][\"gcn_layer_params\"][\"type\"]\n",
    "    hyper_parameters[\"scale\"] = train[\"scale_reward\"]\n",
    "    hyper_parameters[\"qa_function\"] = train[\"qa_function\"]\n",
    "\n",
    "    if \"learning_rate\" in train:\n",
    "        hyper_parameters[\"lr\"] = train[\"learning_rate\"]\n",
    "\n",
    "    return hyper_parameters\n",
    "\n",
    "\n",
    "def nanmean(data):\n",
    "    return None if np.isnan(data).any() else round(np.mean(data))\n",
    "\n",
    "\n",
    "def nanstd(data):\n",
    "    return None if np.isnan(data).any() else round(np.std(data))\n",
    "\n",
    "\n",
    "def nanmax(data):\n",
    "    return None if np.isnan(data).any() else round(np.max(data))\n",
    "\n",
    "\n",
    "def nanmin(data):\n",
    "    return None if np.isnan(data).any() else round(np.min(data))\n",
    "\n",
    "\n",
    "def get_dataframe(\n",
    "    room_size: Literal[\"xxs\", \"xs\", \"s\", \"m\", \"l\", \"xl\", \"xxl\"],\n",
    "    base_path: str = \"training-results/non-equal-object-probs/\",\n",
    ") -> pd.DataFrame:\n",
    "    paths = glob(os.path.join(base_path, f\"dqn/room_size={room_size}/*/*/results.yaml\"))\n",
    "\n",
    "    if len(paths) == 0:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    final = {}\n",
    "    for result_path in tqdm(paths):\n",
    "        result = read_yaml(result_path)\n",
    "        train_path = result_path.replace(\"results.yaml\", \"train.yaml\")\n",
    "        train = read_yaml(train_path)\n",
    "\n",
    "        val_score = max([foo[\"mean\"] for foo in result[\"validation_score\"]])\n",
    "        test_score = result[\"test_score\"][\"mean\"]\n",
    "\n",
    "        hp = determine_hyper_parameters(train)\n",
    "\n",
    "        hp_str = str(hp)\n",
    "        if hp_str in final:\n",
    "            final[hp_str][\"val\"].append(val_score)\n",
    "            final[hp_str][\"test\"].append(test_score)\n",
    "            final[hp_str][\"path\"].append(result_path.split(\"/\")[5].split(\".\")[-1])\n",
    "\n",
    "        else:\n",
    "            final[hp_str] = {\n",
    "                \"val\": [val_score],\n",
    "                \"test\": [test_score],\n",
    "                \"hyper_parameters\": hp,\n",
    "                \"path\": [result_path.split(\"/\")[5].split(\".\")[-1]],\n",
    "            }\n",
    "\n",
    "    df_list = []\n",
    "    for key in final:\n",
    "        data = final[key]\n",
    "        hp = data[\"hyper_parameters\"]\n",
    "        df_list.append(\n",
    "            {\n",
    "                \"test\": nanmean(data[\"test\"]),\n",
    "                # \"std_test\": nanstd(data[\"test\"]),\n",
    "                # \"test_max\": nanmax(data[\"test\"]),\n",
    "                # \"test_min\": nanmin(data[\"test\"]),\n",
    "                \"val\": nanmean(data[\"val\"]),\n",
    "                # \"std_val\": nanstd(data[\"val\"]),\n",
    "                \"#_runs\": len(data[\"test\"]),\n",
    "                \"capacity\": hp.get(\"capacity\", None),\n",
    "                \"agent_type\": hp.get(\"agent_type\", None),\n",
    "                \"pretrain_semantic\": hp.get(\"pretrain_semantic\", None),\n",
    "                \"semantic_decay_factor\": hp.get(\"semantic_decay_factor\", None),\n",
    "                \"room_size\": hp.get(\"room_size\", None),\n",
    "                \"mm\": hp.get(\"mm\", None),\n",
    "                \"qa\": hp.get(\"qa_function\", None),\n",
    "                \"explore\": hp.get(\"explore\", None),\n",
    "                \"#_iter\": hp.get(\"#_iter\", None),\n",
    "                \"replay_size\": hp.get(\"replay_size\", None),\n",
    "                # \"warm_start\": hp.get(\"warm_start\", None),\n",
    "                \"terminates_at\": hp.get(\"terminates_at\", None),\n",
    "                \"target_update\": hp.get(\"target_update\", None),\n",
    "                \"gamma_mm\": hp.get(\"gamma_mm\", None),\n",
    "                \"gamma_explore\": hp.get(\"gamma_explore\", None),\n",
    "                \"relu_between\": hp.get(\"relu_between\", None),\n",
    "                \"dropout_between\": hp.get(\"dropout_between\", None),\n",
    "                \"num_layers\": hp.get(\"num_layers\", None),\n",
    "                \"batch_size\": hp.get(\"batch_size\", None),\n",
    "                \"emb_dim\": hp.get(\"emb_dim\", None),\n",
    "                \"triple_weight\": hp.get(\"triple_weight\", None),\n",
    "                \"intrinsic\": hp.get(\"intrinsic\", None),\n",
    "                \"lr\": hp.get(\"lr\", None),\n",
    "                \"gcn\": hp.get(\"gcn\", None),\n",
    "                \"scale\": hp.get(\"scale\", None),\n",
    "                \"path\": data[\"path\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df = pd.DataFrame(df_list)\n",
    "    df_sorted = df.sort_values(\n",
    "        by=[\"capacity\", \"test\"],\n",
    "        ascending=[True, False],\n",
    "    )\n",
    "    return df_sorted\n",
    "\n",
    "\n",
    "# Function to add blank rows and flag them\n",
    "def add_blank_rows_and_flag(df):\n",
    "    # Create a list to hold the new rows\n",
    "    new_rows = []\n",
    "    previous_capacity = None\n",
    "\n",
    "    # Iterate through the dataframe rows\n",
    "    for index, row in df.iterrows():\n",
    "        if previous_capacity is not None and row[\"capacity\"] != previous_capacity:\n",
    "            # Add a blank row and flag it when the capacity changes\n",
    "            blank_row = pd.Series({col: \"\" for col in df.columns})\n",
    "            blank_row[\"flag\"] = True\n",
    "            new_rows.append(blank_row)\n",
    "        # Append the current row\n",
    "        new_row = row.copy()\n",
    "        new_row[\"flag\"] = False\n",
    "        new_rows.append(new_row)\n",
    "        previous_capacity = row[\"capacity\"]\n",
    "\n",
    "    # Create a new dataframe from the new rows\n",
    "    new_df = pd.DataFrame(new_rows).reset_index(drop=True)\n",
    "    return new_df\n",
    "\n",
    "\n",
    "# Function to highlight the flagged rows\n",
    "def highlight_blank_rows(row):\n",
    "    if row.flag:\n",
    "        return [\"background-color: yellow\"] * len(row)\n",
    "    else:\n",
    "        return [\"\"] * len(row)\n",
    "\n",
    "\n",
    "def get_all_data(\n",
    "    size: Literal[\"xxs\", \"xs\", \"s\", \"m\", \"l\", \"xl\", \"xxl\"],\n",
    "    include_pretrain_semantic: bool = True,\n",
    "    base_path: str = \"training-results/non-equal-object-probs/\",\n",
    "    include_handcrafted: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Get all data.\"\"\"\n",
    "\n",
    "    if include_handcrafted:\n",
    "        df_1 = get_handcrafted(\n",
    "            size,\n",
    "            include_pretrain_semantic=include_pretrain_semantic,\n",
    "            base_path=base_path,\n",
    "        )\n",
    "        df_2 = get_dataframe(size, base_path=base_path)\n",
    "        df = pd.concat([df_1, df_2], ignore_index=True)\n",
    "\n",
    "        df = df.sort_values(\n",
    "            by=[\"capacity\", \"test\"],\n",
    "            ascending=[True, False],\n",
    "        )\n",
    "    else:\n",
    "        df = get_dataframe(size, base_path=base_path)\n",
    "\n",
    "    df.rename(columns={\"semantic_decay_factor\": \"sem_decay\"}, inplace=True)\n",
    "    df.rename(columns={\"pretrain_semantic\": \"pretrain_sem\"}, inplace=True)\n",
    "    df.rename(columns={\"history_block_size\": \"history\"}, inplace=True)\n",
    "\n",
    "    # Add blank rows and flag them in the dataframe\n",
    "    df_with_blanks = add_blank_rows_and_flag(df)\n",
    "\n",
    "    # Apply the highlight function\n",
    "    df_with_blanks_styled = df_with_blanks.style.apply(highlight_blank_rows, axis=1)\n",
    "\n",
    "    # Apply number formatting to styled DataFrame\n",
    "    df_with_blanks_styled = df_with_blanks_styled.format(na_rep=\"NaN\", precision=4)\n",
    "\n",
    "    return df, df_with_blanks_styled\n",
    "\n",
    "\n",
    "def filter_paths(\n",
    "    room_size: Literal[\"xxs\", \"xs\", \"s\", \"m\", \"l\", \"xl\", \"xxl\"],\n",
    "    agent_type: Literal[\"baseline\", \"episodic\", \"semantic\", \"hybrid\"],\n",
    "    num_iterations: int,\n",
    "    capacity: int,\n",
    "    semantic_decay_factor: float,\n",
    "    base_path: str,\n",
    ") -> list:\n",
    "    paths = glob(\n",
    "        os.path.join(base_path, f\"baselines/room_size={room_size}/*/*/results.yaml\")\n",
    "    ) + glob(\n",
    "        os.path.join(base_path, f\"dqn/room_size={room_size}/*/*/explore/results.yaml\")\n",
    "    )\n",
    "\n",
    "    if len(paths) == 0:\n",
    "        return []\n",
    "\n",
    "    filtered_paths = []\n",
    "    for path in tqdm(paths):\n",
    "        if \"baseline\" in path:\n",
    "            train_path = path.replace(\"results.yaml\", \"train.yaml\")\n",
    "        else:\n",
    "            train_path = path.replace(\"explore/results.yaml\", \"train.yaml\")\n",
    "\n",
    "        train_data = read_yaml(train_path)\n",
    "\n",
    "        # Determine hyperparameters to check agent type and capacity\n",
    "        hp = determine_hyper_parameters(train_data)\n",
    "\n",
    "        if hp.get(\"#_iter\") != num_iterations:\n",
    "            continue\n",
    "\n",
    "        if hp.get(\"semantic_decay_factor\") != semantic_decay_factor:\n",
    "            continue\n",
    "\n",
    "        # Check the agent type\n",
    "        if hp.get(\"agent_type\") != agent_type:\n",
    "            continue\n",
    "\n",
    "        # Check the capacity range\n",
    "        if hp.get(\"capacity\") != capacity:\n",
    "            continue\n",
    "\n",
    "        # If all conditions are met, add the path to the filtered list\n",
    "        filtered_paths.append(path)\n",
    "\n",
    "    return filtered_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 43/176 [00:15<00:48,  2.73it/s]"
     ]
    }
   ],
   "source": [
    "# filter_paths(\n",
    "#     room_size=\"xl-different-prob\",\n",
    "#     num_iterations=10000,\n",
    "#     agent_type=\"hybrid\",\n",
    "#     capacity=12,\n",
    "#     semantic_decay_factor=0.99,\n",
    "#     base_path=\"training-results/non-equal-object-probs/\",\n",
    "# )\n",
    "\n",
    "# for foo in filter_paths_by_num_iterations(\"xl-different-prob\"):\n",
    "#     dir_path = '/'.join(foo.split('/')[:-2])\n",
    "#     shutil.rmtree(dir_path)\n",
    "\n",
    "# # save dataframe as markdown\n",
    "# df.to_markdown(\"./hp-tuning/xl-different-prob.md\", index=False)\n",
    "\n",
    "df, df_styled = get_all_data(\n",
    "    \"m-different-prob\",\n",
    "    include_pretrain_semantic=True,\n",
    "    base_path=\"training-results/non-equal-object-probs/\",\n",
    ")\n",
    "df_styled"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "human-memory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
